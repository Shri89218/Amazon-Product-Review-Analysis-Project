{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["WKObGhjVCZSv","wHpp38hR5aSA","mt685xkZ3VZZ","-KXt9EvlDadp","MHjyojwY6vWj","_fmjo-VZ7HzA","1hcwX336EFra","adB7Qq-unMb2","8Hb18bMpyjXK","_HlbemBcsCKi","x5cze8u51k24","01p7dcoQCOf9","pU9dc6VbCp8M","tor1FO7CC7XW","RmbGie0rEnv8","Z5qzuRxzDjH3","EFP27XGbDSTK","veyMZ55MD5lJ","H5o4OthdPg0q","wu6B67NuPm_q","EODsJZoMQHiL","1GoxYphMRwB7","Tnt3K648FscG","TR--UkauF6lM"],"authorship_tag":"ABX9TyOzn2N9dZHhR/rG0UMcJfTQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7450579aace24db5b81e5f9ed7d0b800":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef4fe2c855824af3a732085534f6b3ca","IPY_MODEL_8f4a742db0d54ea6b5a4b7477f364866","IPY_MODEL_d1272577255942bb93c4b79b43f501af"],"layout":"IPY_MODEL_3df9b8cb24484f698e3bee2b1ba48ece"}},"ef4fe2c855824af3a732085534f6b3ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abcc3c561a5047ddaeff07534f3fb1a5","placeholder":"​","style":"IPY_MODEL_372c9913f56e42fb91fdef267f873fa8","value":" 40%"}},"8f4a742db0d54ea6b5a4b7477f364866":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f632bfabdf8d4a9fbbde8aeadcec3662","max":833560,"min":0,"orientation":"horizontal","style":"IPY_MODEL_012c0828c5794120960c3ba5d9905272","value":342959}},"d1272577255942bb93c4b79b43f501af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65cb9538560b42d7b6b6666279e25862","placeholder":"​","style":"IPY_MODEL_869215db4c6346b9a0284c858ff6c55b","value":" 342959/833560 [43:12&lt;53:54, 151.69it/s]"}},"3df9b8cb24484f698e3bee2b1ba48ece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abcc3c561a5047ddaeff07534f3fb1a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"372c9913f56e42fb91fdef267f873fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f632bfabdf8d4a9fbbde8aeadcec3662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"012c0828c5794120960c3ba5d9905272":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65cb9538560b42d7b6b6666279e25862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"869215db4c6346b9a0284c858ff6c55b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"95c5db68"},"source":["# CAPSTONE PROJECT DSFT8 - Digital music"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bz1PiTwfpLvD"},"outputs":[],"source":["# Mounting Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"e801dd3e"},"source":["## &#10148;Problem Statement </br> \n","### <div class=\"alert alert-info\">Thomas, a global market analyst, wishes to develop an automated system to analyze and monitor an enormous number of reviews. By monitoring the entire review history of products, he wishes to analyze tone, language, keywords, and trends over time to provide valuable insights that increase the success rate of existing and new products and marketing campaigns.</div>"]},{"cell_type":"markdown","metadata":{"id":"eDs0kkPcBM89"},"source":["## Introduction\n","\n","Everyday we come across various products in our lives, on the digital medium we swipe across hundreds of product choices under one category. It will be tedious for the customer to make selection. Here comes 'reviews' where customers who have already got that product leave a rating after using them and brief their experience by giving reviews. As we know ratings can be easily sorted and judged whether a product is good or bad. But when it comes to sentence reviews we need to read through every line to make sure the review conveys a positive or negative sense. In the era of artificial intelligence, things like that have got easy with the Natural Langauge Processing(NLP) technology."]},{"cell_type":"markdown","metadata":{"id":"_yrS521S3VZJ"},"source":["## Table of contents\n"," - 1.PREPROCESSING AND CLEANING\n"," - 2.BUSINESS INSIGHTS AND VISUALIZATION \n"," - 3.SENTIMENT ANALYSIS\n"," - 4.TEXT CLASSIFICATION\n"," - 5.TIME SERIES ANALYSIS \n"," - 6.CLUSTERING\n"," - 7.PRODUCT RECOMMENDATION\n"," - 8.PREDICTION OF NEXT PURCHASE DAY\n"," - 9.CONCLUSION\n"]},{"cell_type":"markdown","metadata":{"id":"5L6kF1BE2nRM"},"source":["## &#10148; Requried Libraries</br>"]},{"cell_type":"markdown","metadata":{"id":"UpkSRSReLc85"},"source":["- Importing the required libraries for the project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxpYPUnhpkuK"},"outputs":[],"source":["import json                                        # to work with json file\n","import pandas as pd                                # to work with dataframes\n","import numpy as np                                 # to work with numpy arrays\n","import gzip                                        # to extract work file from zip file\n","import nltk                                        # working with nlp algorithms\n","from nltk.sentiment import SentimentIntensityAnalyzer  # To predict the sentiments based on the text\n","from tqdm.notebook import tqdm                     # library for adding progress bar\n","import sklearn                                     # to working with machine learning algorithms\n","from sklearn.linear_model import LogisticRegression  # Classification algorithm\n","from sklearn.feature_extraction.text import TfidfVectorizer # To convert text to numerical based on tfidf score\n","from nltk.corpus import stopwords                  # to detect stopwords\n","import re                                          # To remove the unwanted text\n","from sklearn.metrics import classification_report  # Classification report\n","from sklearn.metrics import accuracy_score         # evaluation metric\n","from sklearn.metrics import f1_score               # evaluation metric\n","from sklearn.metrics import recall_score           # evaluation metric\n","from sklearn.metrics import precision_score        # evaluation metric\n","from sklearn.model_selection import train_test_split # train test split\n","import time                                        # to check the processing time\n","from sklearn.preprocessing import LabelEncoder     # To convert categorical to numerical\n","import warnings\n","warnings.filterwarnings('ignore')                  # To ignore the warnings\n","from sklearn.model_selection import StratifiedKFold # Splitting\n","from sklearn.naive_bayes import MultinomialNB       # Naive bayes algorithm\n","import matplotlib.pyplot as plt                     # Visualization tool\n","import seaborn as sns                               # Visualization tool\n","from statsmodels.tsa.seasonal import seasonal_decompose            # Time series components\n","from statsmodels.tsa.stattools import adfuller                      # To find the stationarity of the data\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf       # To plot ACF and PACF plots\n","from statsmodels.tsa.arima.model import ARIMA                       # To build the ARIMA model\n","from sklearn.metrics import mean_squared_error                      # To check the mean square error                                      \n","from statsmodels.tsa.statespace.sarimax import SARIMAX              # To build the sarimax model\n","from sklearn.neighbors import NearestNeighbors                      # KNN algorithm\n","from sklearn.metrics.pairwise import cosine_similarity              # \n","import scipy.sparse\n","from scipy.sparse import csr_matrix                                 # Correlation Matrix\n","from scipy.sparse.linalg import svds\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler      # For Scaling the data\n","from sklearn.cluster import KMeans                                  # For Cluster Formation  \n","from sklearn.feature_extraction.text import CountVectorizer         # For Vectorisation\n","from wordcloud import WordCloud, STOPWORDS                          # For Word Cloud\n","from sklearn import metrics                                         # For Matrics Algorithms\n","from sklearn.metrics import classification_report                   # For Classification Evaluation Report\n","from datetime import datetime, timedelta,date\n","from sklearn.metrics import confusion_matrix                        # It shows the tabel of probability values\n","from sklearn.metrics import plot_confusion_matrix                   # plot of confusion matrix\n","from sklearn.metrics import mean_squared_error                      # To check the mean square error "]},{"cell_type":"markdown","metadata":{"id":"mGltNnh522CU"},"source":["## &#10148; Converting file from json to dataframe</br>"]},{"cell_type":"markdown","metadata":{"id":"tVr4TksT3VZM"},"source":["- The gzip module provides the GzipFile class, as well as the open() , compress() and decompress() convenience functions.\n","\n","- The Yield keyword in Python is similar to a return statement used for returning values or objects in Python."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2m26cfH-vy-K"},"outputs":[],"source":["def parse(path):                   # Creating Function\n","  g = gzip.open(path, 'rb')        # opens the compressed format file\n","  for l in g:\n","    yield eval(l)                  # Returns eval(l)\n","\n","def getDF(path):                   # Creating Function getDF\n","  i = 0\n","  df = {}                          # Creating empty dictionary\n","  for d in parse(path):\n","    df[i] = d\n","    i += 1\n","  return pd.DataFrame.from_dict(df, orient='index')         # .from_dict creates DataFrame object from dictionary by columns or by index allowing dtype specification.\n","\n","df = getDF('/content/gdrive/MyDrive/Digital music/meta_Digital_Music.json.gz')"]},{"cell_type":"markdown","metadata":{"id":"4C3yAGRjNGG5"},"source":["## &#10148; Importing the Data</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55Invoa9oKdf"},"outputs":[],"source":["# Checking Shape of the dataset\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjJV1fwd4L5L"},"outputs":[],"source":["# Checking top 5 rows of the data set\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZbiKCYUo_fo"},"outputs":[],"source":["# Renaming the columns \n","columns=['userId', 'productId', 'ratings','timestamp']\n","df3 = pd.read_csv(\"/content/gdrive/MyDrive/Digital music/ratings_Digital_Music.csv\", names=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8k64Si_pThn"},"outputs":[],"source":["# Checking shape of df3\n","df3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSv3LpXfApb7"},"outputs":[],"source":["# importing the data\n","df1 = getDF('/content/gdrive/MyDrive/Digital music/reviews_Digital_Music.json.gz')\n","df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CI901gJfoOrO"},"outputs":[],"source":["# Checking shape of df1\n","df1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSFp_ECPpfMW"},"outputs":[],"source":["df1['userID'] = df3['userId']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8svScmSp1If"},"outputs":[],"source":["df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YJcinnbtk24"},"outputs":[],"source":["# importing the data\n","df2 = getDF('/content/gdrive/MyDrive/Digital music/reviews_Digital_Music_5.json.gz')\n","df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EY3bE9AItxIk"},"outputs":[],"source":["# Checking shape\n","df2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8YHpEsc4zGA"},"outputs":[],"source":["# Feature Selection for data1\n","data1 = df1[['asin', 'reviewText','reviewerName', 'overall', 'unixReviewTime', 'reviewTime', 'userID']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igqLvP2l6wA-"},"outputs":[],"source":["# Feature Selection for data2\n","data2 = df[['asin', 'title', 'categories', 'price', 'brand']]\n","data2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpWNfRs77Ri2"},"outputs":[],"source":["# Merging the data set\n","H_data = pd.merge(data1, data2, on = 'asin')\n","H_data.head()"]},{"cell_type":"markdown","metadata":{"id":"HL41gtunB_Re"},"source":["## &#10148; Data Exploration</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9F83RmVCFw4"},"outputs":[],"source":["# Checking Shape of dataset\n","H_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y6NW2_ICH7N"},"outputs":[],"source":["# Checking description\n","H_data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnmpZi4oCKPb"},"outputs":[],"source":["# Checking information of dataset\n","H_data.info()"]},{"cell_type":"markdown","metadata":{"id":"WKObGhjVCZSv"},"source":["## **Dataset Details**\n","#### This file has reviewer ID , User ID, Reviewer Name, Reviewer text, helpful, Summary(obtained from Reviewer text),Overall Rating on a scale 5, Review time\n","\n","#### Description of columns in the file:\n","\n","reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n","\n","asin - ID of the product, e.g. 0000013714\n","\n","reviewerName - name of the reviewer\n","\n","reviewText - text of the review\n","\n","overall - rating of the product\n","\n","summary - summary of the review\n","\n","unixReviewTime - time of the review (unix time)\n","\n","reviewTime - time of the review (raw)"]},{"cell_type":"markdown","metadata":{"id":"-e9hYtoj3lTE"},"source":["## &#10148; Data Preprocessing</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYjSpGtM6en2"},"outputs":[],"source":["# Removing the duplicates\n","H_data.drop_duplicates([\"reviewText\",\"asin\",\"reviewerName\"], keep = \"last\", inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC83sfwt96n2"},"outputs":[],"source":["# Checking null values\n","(H_data.isnull().sum()*100)/H_data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUBMv8Y5114r"},"outputs":[],"source":["# Imputing 'Unknow' in brand column\n","H_data['brand'].fillna('Unknown', inplace = True)"]},{"cell_type":"code","source":["H_data.drop(['title'], axis = 1, inplace = True)"],"metadata":{"id":"ctnd3gx7gRtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISAmtY-Cqrmj"},"outputs":[],"source":["for i in range(50):\n","  H_data['price'] = H_data['price'].interpolate(method = 'linear', limit = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REhh8pMQIduW"},"outputs":[],"source":["H_data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xN5eVo3F48-5"},"outputs":[],"source":["# Dropping remianing null values\n","H_data.dropna(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMGndQV5_3lH"},"outputs":[],"source":["H_data.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"EI8SRO02317v"},"source":["## &#10148; Data cleaning</br>"]},{"cell_type":"markdown","metadata":{"id":"emfqw2gB3VZW"},"source":["- Clean text is human language rearranged into a format that machine models can understand. Text cleaning can be performed using simple Python code that eliminates stopwords, removes unicode words, and simplifies complex words to their root form."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3SFcxX16Ana"},"outputs":[],"source":["# Creating cleaning function\n","import re\n","def cleaning(text):\n","    text = re.sub(\"[^0-9A-Za-z\\-]+\", \" \", text) \n","    text = re.sub(\"(?<!\\w)\\d+\", \"\", text)\n","    text = re.sub(\"-(?!\\w)\", \"\", text)\n","    text = \" \".join(text.split())\n","    text = text.lower()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZVXitrE7jZk"},"outputs":[],"source":["# Calling the cleaning function for reviewText column\n","H_data[\"reviewText\"] = H_data[\"reviewText\"].apply(cleaning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MrnD5J59o5w"},"outputs":[],"source":["# Checking Information\n","H_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6Yief_V9zLh"},"outputs":[],"source":["# converting the data type of reviewTime with date type\n","H_data['reviewTime'] = pd.to_datetime(H_data['reviewTime'])"]},{"cell_type":"markdown","metadata":{"id":"wHpp38hR5aSA"},"source":["## &#10148; Sentiment Analysis</br>"]},{"cell_type":"markdown","metadata":{"id":"lyQ9j64G6fFg"},"source":["## What is sentiment analysis?\n","\n","- Sentiment analysis is a text analysis method that detects polarity (e.g. a positive or negative opinion) within the text, whether a whole document, paragraph, sentence, or clause.\n","- Sentiment analysis aims to measure the attitude, sentiments, evaluations, attitudes, and emotions of a speaker/writer based on the computational treatment of subjectivity in a text.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nBziq9jT6UBs"},"source":["### Creating 'sentiment' column\n","This is an important preprocessing phase, we are deciding the outcome column (sentiment of review) based on the overall score. If the score is greater than 3, we take that as positive and if the value is less than 3 it is negative If it is equal to 3, we take that as neutral sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rM0eL14AD9e"},"outputs":[],"source":["# Assigning the Positive Negative and Neutral Sentiment ob the basis of overall column\n","a=[]\n","for x in H_data['overall']: \n","  if x>3:\n","    x='Pos'\n","    a.append(x)\n","  elif x==3:\n","    x='Neutral'\n","    a.append(x)\n","  else:\n","    x='Neg'\n","    a.append(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeg9oRmS_vlq"},"outputs":[],"source":["H_data['Sentiment']=a\n"]},{"cell_type":"markdown","metadata":{"id":"mt685xkZ3VZZ"},"source":["#### VADER\n","- VADER ( Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is available in the NLTK package and can be applied directly to unlabeled text data.\n","- VADER sentimental analysis relies on a dictionary that maps lexical features to emotion intensities known as sentiment scores. The sentiment score of a text can be obtained by summing up the intensity of each word in the text.\n","- For example- Words like ‘love’, ‘enjoy’, ‘happy’, ‘like’ all convey a positive sentiment. Also VADER is intelligent enough to understand the basic context of these words, such as “did not love” as a negative statement. It also understands the emphasis of capitalization and punctuation, such as “ENJOY”"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlLxTmpM5Tr7"},"outputs":[],"source":["# downloding the vader lexicon \n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6F4TEkEI6HBV","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["7450579aace24db5b81e5f9ed7d0b800","ef4fe2c855824af3a732085534f6b3ca","8f4a742db0d54ea6b5a4b7477f364866","d1272577255942bb93c4b79b43f501af","3df9b8cb24484f698e3bee2b1ba48ece","abcc3c561a5047ddaeff07534f3fb1a5","372c9913f56e42fb91fdef267f873fa8","f632bfabdf8d4a9fbbde8aeadcec3662","012c0828c5794120960c3ba5d9905272","65cb9538560b42d7b6b6666279e25862","869215db4c6346b9a0284c858ff6c55b"]},"outputId":"262875fb-1d9c-406f-e718-5277b72ab85a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7450579aace24db5b81e5f9ed7d0b800","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/833560 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Getting the polarity of reviewText \n","res2 = {}\n","t = 0\n","for i, row in tqdm(H_data.iterrows(), total=len(H_data)):\n","    text = row['reviewText']\n","    res2[t] = SentimentIntensityAnalyzer().polarity_scores(text)\n","    t = t + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuUWMyTg6swv"},"outputs":[],"source":["# Transposing the dataframe\n","j = pd.DataFrame(res2).T\n","j"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDVbEzb26szN"},"outputs":[],"source":["# concating the main data and the Polarity Scores\n","M_data = pd.concat([H_data, j], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nC0CTPSR8qD3"},"outputs":[],"source":["# Dropping the null values\n","M_data.dropna(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_9qz2al6s10"},"outputs":[],"source":["# Creating the Class column based on compound column\n","M_data.insert(0, 'Class', np.nan)\n","M_data.loc[M_data['compound']>=0.05, 'Class'] = 'pos'\n","M_data.loc[M_data['compound']<=-0.05, 'Class'] = 'neg'\n","M_data.loc[((M_data['compound'] > -0.05) & (M_data['compound'] < 0.05)), 'Class'] = 'neutral'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0WrLFs78_zH"},"outputs":[],"source":["# giving the datetiem index for reviewtime on the basis of year and month\n","M_data['year'] = pd.DatetimeIndex(M_data['reviewTime']).year\n","M_data['month'] = pd.DatetimeIndex(M_data['reviewTime']).month"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4vAqC446s4C"},"outputs":[],"source":["# converting thedata into CSV file\n","M_data.to_csv(\"M_datafinal2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnnDq-byAGki"},"outputs":[],"source":["# installing the googletrans library\n","!pip install googletrans==3.1.0a0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYPxy1p-Cjgq"},"outputs":[],"source":["# Importing the GoogleTrans library\n","from googletrans import Translator\n","translator = Translator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6m7sW0vAGm7"},"outputs":[],"source":["text1 = '''\n","A Római Birodalom (latinul Imperium Romanum) az ókori Róma által létrehozott \n","államalakulat volt a Földközi-tenger medencéjében\n","'''\n","\n","text2 = '''\n","Vysoké Tatry sú najvyššie pohorie na Slovensku a v Poľsku a sú zároveň jediným \n","horstvom v týchto štátoch s alpským charakterom. \n","'''\n","a = [text1, text2]\n","a = pd.DataFrame({'col':a})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BV52TJQ5kFbH"},"outputs":[],"source":["# Creating a loop to check if the language is english or not, if not translating it into english\n","for i in range(len(a.iloc[:, 0])):\n","  dt = translator.detect(a.iloc[i, 0])\n","  if  dt != 'en':\n","    a.iloc[i, 0] = translator.translate(a.iloc[i, 0],dest='en').text"]},{"cell_type":"markdown","metadata":{"id":"pExuCGT0_3i_"},"source":["## &#10148; Text classification</br>\n","- Text classification also known as text tagging or text categorization is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMRCh1YD6s6z"},"outputs":[],"source":["# importing the data set which we have created\n","df = pd.read_csv('/content/gdrive/MyDrive/CSV files/M_datafinal2.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EohWiRxA2uiO"},"outputs":[],"source":["# Converting the datatype of reviewTime to Date type\n","df[\"reviewTime\"] = pd.to_datetime(df[\"reviewTime\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_q3S-sQP6s97"},"outputs":[],"source":["# checking null values\n","df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWaEYm95HJ5I"},"outputs":[],"source":["# dropping the null values\n","df.dropna(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQpovqXT6s_T"},"outputs":[],"source":["# dropping the 'Unnamed: 0' column\n","df.drop('Unnamed: 0', axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7jV-hb8U4tF"},"outputs":[],"source":["# slicing the data\n","df1 = df.iloc[:100, :]\n","df1.head()"]},{"cell_type":"markdown","metadata":{"id":"-KXt9EvlDadp"},"source":["### Remove text-Stop words\n","Coming to stop words, general nltk stop words contains words like not,hasn't,would'nt which actually conveys a negative sentiment. If we remove that it will end up contradicting the target variable(sentiment). So I have curated the stop words which doesn't have any negative sentiment or any negative alternatives."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f9MBtBESfXc"},"outputs":[],"source":["# Getting stop words\n","nltk.download('stopwords')\n","\n","stop_words = stopwords.words(\"english\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"galc8ldZS_uP"},"outputs":[],"source":["# applying stopword function on reviewText\n","df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsLzN1wuqoU9"},"outputs":[],"source":["# For WordCloud\n","stopwords = set(STOPWORDS)\n","def word_cloud(data, title):\n","    wordcloud = WordCloud(\n","    background_color = \"black\",\n","    max_font_size = 40,\n","    max_words = 200,\n","    stopwords = stopwords,\n","    scale = 3).generate(str(df['reviewText']))\n","    fig = plt.figure(figsize = (15, 15))\n","    plt.axis(\"off\")\n","    if title: \n","        fig.suptitle(title, fontsize=15)\n","        fig.subplots_adjust(top=2.25)\n","    plt.imshow(wordcloud)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wW-OZ3Q7xoLl"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x051hv-zxkUx"},"outputs":[],"source":["neg=df[df[\"Pros_cons\"] == \"Neg\"][\"reviewText\"]\n","pos=df[df[\"Pros_cons\"] == \"Pos\"][\"reviewText\"]\n","neu=df[df[\"Pros_cons\"] == \"Neutral\"][\"reviewText\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIKHmg4kyAxA"},"outputs":[],"source":["word_cloud(pos, \"Most Repeated words in positive reviews\")\n","word_cloud(neg, \"Most Repeated words in negative reviews\")\n","word_cloud(neu, \"Most Repeated words in neutral reviews\")"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From the above plots we can see that most used positive, negative and neutral words from the review text**</div>"],"metadata":{"id":"MHjyojwY6vWj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVoAH7Deytol"},"outputs":[],"source":["# Getting BIGRAM\n","def get_top_n_bigram(corpus, n=None):\n","    vec = CountVectorizer(ngram_range=(2, 2),stop_words='english').fit(corpus)   # converting a text documents to a matrix of token counts.      \n","    bag_of_words = vec.transform(corpus)                                         # Transforming the corpus into numbers\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]     # it provides a dictionary with the mapping of the word item index \n","    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n","    return words_freq[:n]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyHcM040qoX1"},"outputs":[],"source":["# create a function for bigram plots\n","def n_gram_plot(data,title,color):                            # Creating n_gram_plot function\n","    x=[x[0] for x in data]\n","    y=[x[1] for x in data]\n","    sns.barplot(y,x,color='{}'.format(color))\n","    plt.title('{} Reviews Bigrams'.format(title),fontsize=15)\n","    plt.yticks(rotation=0,fontsize=15)\n","\n","common_words_good = get_top_n_bigram(pos, 10)                  # Calling get_top_n_bigram for pos columns\n","common_words_neutral = get_top_n_bigram(neu, 10)               # Calling get_top_n_bigram for neu columns\n","common_words_bad = get_top_n_bigram(neg, 10)                   # Calling get_top_n_bigram for neg columns\n","\n","# bigram plot using function above\n","plt.figure(figsize=(15,10))\n","# good reviews bigrams\n","plt.subplot(151)\n","n_gram_plot(common_words_good,'Good','green')                  # Calling n_gram_plot for pos \n","#============================================= \n","#neutral reviews bigrams\n","plt.subplot(153)\n","n_gram_plot(common_words_neutral,'Neutral','yellow')           # Calling n_gram_plot for pos\n","#============================================= \n","#bad reviews bigrams\n","plt.subplot(155)\n","n_gram_plot(common_words_bad,'Bad','red')                      # Calling n_gram_plot for pos\n","plt.show()"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","- **From the above plots we can see that most occuring bigram words in the text reveiws**</div>"],"metadata":{"id":"_fmjo-VZ7HzA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3P4YXQRAGft"},"outputs":[],"source":["X = df['reviewText']\n","Y = df['Pros_cons']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pi_dugCNpVpC"},"outputs":[],"source":["Y.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiLhCxM7AGh9"},"outputs":[],"source":["Y = LabelEncoder().fit_transform(Y)\n","Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsNAXzIuyfh0"},"outputs":[],"source":["# Getting unique values and converting it into array\n","unique, counts = np.unique(Y, return_counts=True)\n","print(np.asarray((unique, counts)).T)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgFnLbuhowvL"},"outputs":[],"source":["# Splitting the data\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cSCbPQustZq"},"outputs":[],"source":["unique, counts = np.unique(Y_train, return_counts=True)\n","print(np.asarray((unique, counts)).T)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPc-FJFbm6Kp"},"outputs":[],"source":["# Applying TFIDF Vectorizer\n","%%time\n","vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.1, min_df = 1,\n","                             use_idf = True, smooth_idf = True)\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"1hcwX336EFra"},"source":["## Model selection\n","Let's consider all the classification algorithm and perform the model selection process"]},{"cell_type":"markdown","metadata":{"id":"adB7Qq-unMb2"},"source":["#### &#10148; Logistic regression</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eLay4zsmiK6"},"outputs":[],"source":["# Making and Fitting the Model\n","%%time\n","model = LogisticRegression(multi_class = 'ovr').fit(X_train, Y_train)\n","y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-agojHYkuwJT"},"outputs":[],"source":["# Making unique Values and converting the values in array \n","unique, counts = np.unique(y_pred, return_counts=True)\n","print(np.asarray((unique, counts)).T)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhdB2Jzbnkqc"},"outputs":[],"source":["%%time\n","print(classification_report(Y_test, y_pred, target_names = ['neg', 'neu', 'pos']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyjAeHJvt5UD"},"outputs":[],"source":["color = 'white'\n","fig, ax = plt.subplots(figsize=(10, 10))\n","plot_confusion_matrix(model, X_test, Y_test, cmap=plt.cm.Blues, display_labels = ['Negative','Neutral','Positive'], ax = ax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8Hb18bMpyjXK"},"source":["#### &#10148; Sample Illustration</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqZ7N2QlrskN"},"outputs":[],"source":["a = ['Nice song to here', 'worst song and waste of money', 'Good song but quality is not good']\n","a1 = vectorizer.transform(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpGArH1_pPYd"},"outputs":[],"source":["fo = model.predict(a1)\n","fo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MO4RisHQtfC-"},"outputs":[],"source":["s = pd.DataFrame({\"Random_review\":a, \"Predictions\": ['Positive', 'Negative', 'Positive']})\n","s"]},{"cell_type":"markdown","metadata":{"id":"_HlbemBcsCKi"},"source":["#### &#10148; Naive bayes classifier</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OYWzHNZsNvy"},"outputs":[],"source":["# Making and Fitting the model\n","%%time\n","model1 = MultinomialNB().fit(X_train, Y_train)\n","y_pred1 = model1.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdqkzPx0sNx9"},"outputs":[],"source":["print(classification_report(Y_test, y_pred1, target_names = ['neg', 'nue', 'pos']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvWQVbjavAGB"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 10))\n","plot_confusion_matrix(model1, X_test, Y_test, cmap=plt.cm.Blues, display_labels = ['Negative','Neutral','Positive'], ax = ax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x5cze8u51k24"},"source":["## &#10148; Time series analysis</br>\n","- Time series analysis is a technique in statistics that deals with time series data and trend analysis. Time series data follows periodic time intervals that have been measured in regular time intervals or have been collected in particular time intervals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pe3gu0g6sN0J"},"outputs":[],"source":["%%time\n","plt.figure(figsize = (15, 8))\n","plt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\n","sns.kdeplot(df['neg'],bw=0.1,cumulative=True)\n","sns.kdeplot(df['neu'],bw=0.1,cumulative=True)\n","sns.kdeplot(df['pos'],bw=0.1,cumulative=True)\n","plt.xlabel('Sentiment Value',fontsize=19)\n","plt.legend(['neg', 'neutral', 'pos'])\n","plt.show()"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","- **It is also clear that the dominant sentiment is neutral; oddly, most of the reviews do not resemble more positive or negative sentiment rather than neutral.**</div>"],"metadata":{"id":"01p7dcoQCOf9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZs_bA3OsN2a"},"outputs":[],"source":["df1 = df[['neg', 'pos', 'reviewTime']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRpAqQ9psN4P"},"outputs":[],"source":["# Setting the index as Date\n","df1 = df1.set_index('reviewTime')\n","df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y97KpJRLsN6T"},"outputs":[],"source":["# Resampling the data based on Weekly \n","df1 = df1.resample('W').sum()\n","df1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNlKJ8tD27OM"},"outputs":[],"source":["# Seasonal Decompose For Positive Reviews\n","%%time\n","decomposition=seasonal_decompose(df1['pos'], period=52)\n","d_trend=decomposition.trend\n","d_seasonal=decomposition.seasonal\n","d_residual=decomposition.resid\n","\n","\n","fig,ax = plt.subplots(4,2,figsize=(30,20))\n","\n","plt.subplot(411)\n","plt.plot(df1['pos'],label='Original')\n","plt.legend(loc='best')\n","plt.title('Pos_actual', fontsize = 20)\n","\n","plt.subplot(412)\n","plt.plot(d_trend,label='Trend')\n","plt.legend(loc='best')\n","plt.title('Pos_trend', fontsize = 20)\n","\n","plt.subplot(413)\n","plt.plot(d_seasonal,label='Seasonal')\n","plt.legend(loc='best')\n","plt.title('Pos_seasonal', fontsize = 20)\n","\n","plt.subplot(414)\n","plt.plot(d_residual,label='Residual')\n","plt.legend(loc='best')\n","plt.title('Pos_residual', fontsize = 20)"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","- **Above plots shows the time series decomposition and we can see the components of time series for positive reviews**\n","- **First plot is actual data plot with weekly dispersed data points**\n","- **Second one shows the trend in the data where we can see there is positive trend in the dataset**\n","- **Third plot shows the seasonality which we can see some repetitions over the period of time**\n","- **Last plot shows the irregularity in the the data over a period of time we can see some irregular kind over a period in the plot**"],"metadata":{"id":"pU9dc6VbCp8M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKTGwQrX308t"},"outputs":[],"source":["# Seasonal Decompose For Negative Reviews\n","%%time\n","decomposition=seasonal_decompose(df1['neg'], period=52)\n","d_trend=decomposition.trend\n","d_seasonal=decomposition.seasonal\n","d_residual=decomposition.resid\n","\n","\n","fig,ax = plt.subplots(4,2,figsize=(30,20))\n","\n","plt.subplot(411)\n","plt.plot(df1['neg'],label='Original')\n","plt.legend(loc='best')\n","plt.title('neg_actual', fontsize = 20)\n","\n","plt.subplot(412)\n","plt.plot(d_trend,label='Trend')\n","plt.legend(loc='best')\n","plt.title('neg_trend', fontsize = 20)\n","\n","plt.subplot(413)\n","plt.plot(d_seasonal,label='Seasonal')\n","plt.legend(loc='best')\n","plt.title('neg_seasonal', fontsize = 20)\n","\n","plt.subplot(414)\n","plt.plot(d_residual,label='Residual')\n","plt.legend(loc='best')\n","plt.title('neg_residual', fontsize = 20)"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","- **Above plots shows the time series decomposition and we can see the components of time series for negative reviews**\n","- **First plot is actual data plot with weekly dispersed data points**\n","- **Second one shows the trend in the data where we can see there is positive trend in the dataset**\n","- **Third plot shows the seasonality which we can see some repetitions over the period of time**\n","- **Last plot shows the irregularity in the the data over a period of time we can see some irregular kind over a period in the plot**"],"metadata":{"id":"tor1FO7CC7XW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryttLsLW3sQT"},"outputs":[],"source":["plt.figure(figsize=(15,12))\n","plt.subplot(211)\n","plot_acf(df['pos'], ax=plt.gca(), lags = 52)\n","plt.subplot(212)\n","plot_pacf(df['pos'], ax=plt.gca(), lags = 52)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YFLTRx64wxN"},"outputs":[],"source":["plt.figure(figsize=(15,12))\n","plt.subplot(211)\n","plot_acf(df['neg'], ax=plt.gca(), lags = 52)\n","plt.subplot(212)\n","plot_pacf(df['neg'], ax=plt.gca(), lags = 52)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J5d-QnbJlCs"},"outputs":[],"source":["plt.figure(figsize=(25, 6))\n","plt.subplot(1, 2, 1)\n","sns.boxplot(df.year, df.pos)\n","\n","plt.subplot(1, 2, 2)\n","sns.boxplot(df.month, df.pos)\n","plt.show()"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From the visualisation we can see how the sentiments distributed over the years as well as months, the mean and the variation is almost same over the period**</div>"],"metadata":{"id":"RmbGie0rEnv8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6ez03pY4Af4"},"outputs":[],"source":["plt.figure(figsize=(25, 6))\n","sns.countplot(df.year)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yCG5xk8KYP0"},"outputs":[],"source":["plt.figure(figsize=(25, 6))\n","sns.countplot(df.month)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3wJp5jxKYTe"},"outputs":[],"source":["# Creating function to check stationarity\n","def checkstationary(df):\n","    pvalue = adfuller(df)[1]\n","    if pvalue < 0.05:\n","        ret = 'Pvalue:{}. Data is stationary, Proceed to model building'.format(pvalue)\n","    else:\n","        ret = 'Pvalue:{}.Data is not stationary, make data stationary'.format(pvalue)\n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vGug_t1KYWB"},"outputs":[],"source":["# Checking Stationarity of Negative Sentiment Column\n","checkstationary(df1['neg'])"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From augmented dickey fuller test we can see that the data is not stationary so we should do diffrencing or d = 1 while building the model**</div>"],"metadata":{"id":"Z5qzuRxzDjH3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_MyFAawKYYy"},"outputs":[],"source":["# Checking Stationarity of Positive Sentiment Column\n","checkstationary(df1['pos'])"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From augmented dickey fuller test we can see that the data is not stationary so we should do diffrencing or d = 1 while building the model**</div>"],"metadata":{"id":"EFP27XGbDSTK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lT-nGQhkKYaj"},"outputs":[],"source":["# Splitting the data\n","split = int(0.95 * len(df1))\n","train = df1.iloc[:split]\n","test = df1.iloc[split:]\n","print(\"Train = {}, Test = {}\".format(len(train), len(test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI-J--CvKqpN"},"outputs":[],"source":["# Creating function to get optimum p and q value\n","def sarima_model(p,d,q,P,D,Q):\n","    sm1=SARIMAX(train,order=(p,d,q),seasonal_order=(P,D,Q,52)).fit()\n","    f1=sm1.forecast(len(test))\n","    actual=[]\n","    predicted=[]\n","    for i in range(len(f1)):\n","        actual.append(test[i])\n","        predicted.append(f1[i])\n","    RMSE=round(mean_squared_error(actual,predicted,squared=False),3)\n","    return RMSE,actual,predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plzUATD0KquZ"},"outputs":[],"source":["p=[0,1, 2]\n","d=1\n","q=[0,1, 2]\n","p1=[]\n","q1=[]\n","rmse1=[]\n","P=[0,1, 2]\n","Q=[0,1, 2]\n","D=1\n","P1=[]\n","Q1=[]\n","for i in range(len(p)):\n","    for j in range(len(q)):\n","        for k in range(len(P)):\n","            for l in range(len(Q)):\n","                p1.append(p[i])\n","                q1.append(q[j])\n","                P1.append(P[k])\n","                Q1.append(Q[l])\n","                rmse1.append(sarima_model(p[i],d,q[j],P[k],D,Q[l])[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"562YZV4xKqwc"},"outputs":[],"source":["val2 = pd.DataFrame(zip(p1,q1,P1,Q1,rmse1),columns=['p','q','P','Q','RMSE'])\n","val2.sort_values(by='RMSE').head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHQnycnLKqzB"},"outputs":[],"source":["# Creating function for sarima model for negative sentiment\n","def SARMA1(df):\n","    model2 = SARIMAX(train['neg'],order=(1, 1, 2),seasonal_order=(1,1,2,52)).fit()\n","    print('Summary : S')\n","    print('past_predictions : past')\n","    print('future_predictions : future')\n","    select = input('Enter you requried information: ')\n","    summary  = model2.summary()\n","    pred1 = model2.predict()\n","    forecast1 = model2.forecast(len(test['neg'])+20)\n","    if select == 'S':\n","        return summary\n","    elif select == 'past':\n","        return pred1\n","    else:\n","        return forecast1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhKToYToKq07"},"outputs":[],"source":["# Creating function for sarima model for positive sentiment\n","def SARMA2(df):\n","    model2 = SARIMAX(train['pos'],order=(1, 1, 2),seasonal_order=(1,1,2,52)).fit()\n","    print('Summary : S')\n","    print('past_predictions : past')\n","    print('future_predictions : future')\n","    select = input('Enter you requried information: ')\n","    summary  = model2.summary()\n","    pred1 = model2.predict()\n","    forecast1 = model2.forecast(len(test['pos'])+20)\n","    if select == 'S':\n","        return summary\n","    elif select == 'past':\n","        return pred1\n","    else:\n","        return forecast1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccG-dEICK6Wz"},"outputs":[],"source":["train1 = SARMA1(train['neg'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDXDUrPyK6ZW"},"outputs":[],"source":["train2 = SARMA2(train['pos'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xD3EoMyHK6cl"},"outputs":[],"source":["plt.figure(figsize=(30,10))\n","plt.title('Actual vs forecast')\n","plt.plot(train['neg'],marker = '.', label = 'neg', color = 'red')\n","plt.plot(train['pos'],marker = '.', label = 'pos', color = 'g')\n","plt.plot(train1,marker = '.', label = 'neg_forecast', color = 'b')\n","plt.plot(train2,marker = '.', label = 'pos_forecast', color = 'b')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KeWrETcKYdQ"},"outputs":[],"source":["# Evaluation using RMSE\n","pos_rmse = np.sqrt(mean_squared_error(test['pos'], train2[:-20]))\n","neg_rmse = np.sqrt(mean_squared_error(test['pos'], train1[:-20]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qW9Q4Cr0zGYS"},"outputs":[],"source":["res = pd.DataFrame({'Sentiments':['Pos', 'neg'], 'RMSE':[pos_rmse, neg_rmse]})\n","res"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From both visualization as well as the error values of forcast data we can see the SARIMA model is giving good forcast results and the positive and negative reviews are increasing over period but the positive reviews are inresing in more percent compare to negative review**</div>"],"metadata":{"id":"veyMZ55MD5lJ"}},{"cell_type":"markdown","metadata":{"id":"ws5hafxB4J7w"},"source":["## &#10148; Clustering</br>\n","- Cluster analysis is the grouping of objects such that objects in the same cluster are more similar to each other than they are to objects in another cluster. The classification into clusters is done using criteria such as smallest distances, density of data points, graphs, or various statistical distributions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExdUhhez8YUJ"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6ItbRMA6umw"},"outputs":[],"source":["X1 = df[['price', 'unixReviewTime']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEMCgwi66upN"},"outputs":[],"source":["# Scaling the data\n","%%time\n","Scaler = StandardScaler()\n","for i in X1.columns:\n","    X1[i] = Scaler.fit_transform(np.array(X1[i]).reshape(-1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8csQN-g6uru"},"outputs":[],"source":["%%time\n","X1 = X1.values\n","distortion = []\n","for i in range(2, 10):\n","    kmeans = KMeans(n_clusters = i).fit(X1)\n","    distortion.append(kmeans.inertia_)\n","plt.figure(figsize = (15, 5))\n","plt.plot(range(2, 10), distortion)\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyTm930X_ijC"},"outputs":[],"source":["# Making the model and fitting it\n","%%time\n","model1 = KMeans(n_clusters = 5, random_state = 10).fit(X1)\n","pred = model1.fit_predict(X1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EGSa8EuAKtz"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","source":["plt.figure(figsize=(15,8))\n","sns.scatterplot(x=X1[pred==0,0] ,y=X1[pred==0,1] ,s=100,label=\"q_target\")\n","sns.scatterplot(x=X1[pred==1,0] ,y=X1[pred==1,1],s=100,label=\"Sec_target\")\n","sns.scatterplot(x=X1[pred==2,0] ,y=X1[pred==2,1] ,s=100,label=\"standard\")\n","sns.scatterplot(x=X1[pred==3,0] ,y=X1[pred==3,1],s=100,label=\"target\")\n","sns.scatterplot(x=X1[pred==4,0] ,y=X1[pred==4,1],s=100,label=\"tert_target\")\n","#sns.scatterplot(x=kmeans.cluster_centers_[:,0] ,y= kmeans.cluster_centers_[:,1] ,s=300,label=\"center\")\n","plt.title(\"clusters of customers\")\n","plt.xlabel(\"Price\")\n","plt.ylabel(\"Unixreviewtime\")\n"],"metadata":{"id":"y_eus5AbSjrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","- **1. This graph illustrates the relation between the review time and the price of the product where each cluster shows the grouping of reviews on the particular products**\n","\n","- **2. The purple cluster shows the target cluster which can be said as excellent and the worst reviews that is 5 and 1 rated reviews. As the price of product is increasing the review time is also increasing, from this we can interpret that either the product is very good or worst as people tends to give instant reviews when the product is strongly liked or disliked by them**\n","\n","- **3. The more the review time is increasing we can see the product price is also increasing from which we can interpret that customer take some time to give reviews when the product have higher price**</div>"],"metadata":{"id":"kSs-ZxoFNfhj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mStgXxsWhfEu"},"outputs":[],"source":["clus = df.copy()"]},{"cell_type":"code","source":["# Assigning the clusters \n","clusters=[]\n","for c in pred:\n","    if c==0:\n","        clusters.append(\"q_target\")\n","    elif c==1:\n","        clusters.append(\"Sec_target\")\n","    elif c==2:\n","        clusters.append(\"standard\")\n","    elif c==3:\n","        clusters.append(\"tert_target\")\n","    elif c==4:\n","        clusters.append(\"target\") \n","        \n","        \n","clus[\"clusters1\"]=clusters\n"],"metadata":{"id":"8L4sVf1ETfHf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d1=clus[(clus[\"clusters1\"]=='target')]\n","d1[\"overall\"].value_counts()"],"metadata":{"id":"PFc95-3O27tF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d2=clus[(clus[\"clusters1\"]=='Sec_target')]\n","d2[\"overall\"].value_counts()"],"metadata":{"id":"snwt7fUb27v8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d3=clus[(clus[\"clusters1\"]=='standard')]\n","d3[\"overall\"].value_counts()"],"metadata":{"id":"Ha_9YS3j27yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d4=clus[(clus[\"clusters1\"]=='tert_target')]\n","d4[\"overall\"].value_counts()"],"metadata":{"id":"z88r6Dhi271G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d5=clus[(clus[\"clusters1\"]=='q_target')]\n","d5[\"overall\"].value_counts()"],"metadata":{"id":"cEav9FUf273z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p7HXD1yr276B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w_RcK8qM28F5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2uN6Zjb028JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ueso6EmEoreP"},"outputs":[],"source":["improve=clus[(clus[\"clusters1\"]=='target') & (clus[\"overall\"]<3)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oJz5jy0tXxV"},"outputs":[],"source":["a=clus[(clus[\"clusters1\"]=='target')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5e-ASzWBtcNH"},"outputs":[],"source":["# Recommending the products\n","improve['asin'].value_counts()[0:10]"]},{"cell_type":"code","source":["improve1=clus[(clus[\"clusters1\"]=='target') & (clus[\"overall\"]==1)]\n","improve1['asin'].value_counts()[0:10]"],"metadata":{"id":"wwon1-F1ZbYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["improve2=clus[(clus[\"clusters1\"]=='target') & (clus[\"overall\"]==5)]\n","improve2['asin'].value_counts()[0:10]"],"metadata":{"id":"ppg7tGi3ZzzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9FfUa4ziiTYL"},"source":["## &#10148; Customer segmentation</br>\n","- We can’t treat every customer the same way with the same content, same channel, same importance. They will find another option which understands them better.\n","- Customers who use your platform have different needs and they have their own different profile. Your should adapt your actions depending on that.\n","- You can do many different segmentations according to what you are trying to achieve. If you want to increase retention rate, you can do a segmentation based on the similarities between the customers\n","- But there are very common and useful segmentation methods as well. Now we are going to implement one of them to our business: RFM.\n","- **1. Recency: How recently customers made their purchase.**\n","- **2. Frequency: For simplicity, we’ll count the number of times each customer made a purchase.**\n","- **3. Monetary: How much money they spent in total.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHXnMBl4nwu1"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SVxyplKLaiS"},"outputs":[],"source":["CS_df = pd.DataFrame(df['userID'].unique())\n","CS_df.columns = ['userID']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR1N8gGVMPoS"},"outputs":[],"source":["Max_purchase = df.groupby('userID').reviewTime.max().reset_index()\n","Max_purchase.columns = ['userID','MaxPurchaseDate']"]},{"cell_type":"markdown","metadata":{"id":"H5o4OthdPg0q"},"source":["#### &#10148; Recency</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"va6XsGowNBHK"},"outputs":[],"source":["Max_purchase['Recency'] = (Max_purchase['MaxPurchaseDate'].max() - Max_purchase['MaxPurchaseDate']).dt.days"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qzi3b2rbNWuy"},"outputs":[],"source":["CS_df = pd.merge(CS_df, Max_purchase[['userID','Recency']], on='userID')\n","CS_df.head()"]},{"cell_type":"markdown","metadata":{"id":"wu6B67NuPm_q"},"source":["#### &#10148; Frequency</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDyp1VBDN2PY"},"outputs":[],"source":["tx_frequency = df.groupby('userID').reviewTime.count().reset_index()\n","tx_frequency.columns = ['userID','Frequency']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfYV881VN2Rq"},"outputs":[],"source":["CS_df = pd.merge(CS_df, tx_frequency, on='userID')"]},{"cell_type":"markdown","metadata":{"id":"EODsJZoMQHiL"},"source":["#### &#10148; Revenue</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOQ4XXfQN2UP"},"outputs":[],"source":["tx_revenue = df.groupby('userID').price.sum().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQlBJspvN2W2"},"outputs":[],"source":["CS_df = pd.merge(CS_df, tx_revenue, on='userID')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbR2Q63iRyQ5"},"outputs":[],"source":["CS_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1sFvQXCVO-O"},"outputs":[],"source":["CS_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"1GoxYphMRwB7"},"source":["#### &#10148; K_means</br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCRdK0amT_SI"},"outputs":[],"source":["a = CS_df.select_dtypes(exclude = 'object')\n","b = CS_df.select_dtypes(include = 'object')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDtHiBz6Vekj"},"outputs":[],"source":["a.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2UQwJcfVuVy"},"outputs":[],"source":["CS_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb9iIroiVhUj"},"outputs":[],"source":["CS_df1 = CS_df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bXG1ArwV74t"},"outputs":[],"source":["CS_df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_fcw_gKUUIT"},"outputs":[],"source":["# Scaling the data\n","%time\n","Scaler = StandardScaler()\n","for i in a.columns:\n","    CS_df1[i] = Scaler.fit_transform(np.array(CS_df[i]).reshape(-1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"injAwszNWRcL"},"outputs":[],"source":["# Getting optimum cluster number\n","%%time\n","X = CS_df1.drop(['userID'], axis = 1).values\n","distortion = []\n","for i in range(2, 10):\n","    kmeans = KMeans(n_clusters = i).fit(X)\n","    distortion.append(kmeans.inertia_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppdoPC-KXaEF"},"outputs":[],"source":["plt.figure(figsize = (15, 5))\n","plt.plot(range(2, 10), distortion)\n","plt.grid(True)"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From the above elbow curve we can take k as 3 because the slope is more at k = 3**</div>"],"metadata":{"id":"Tnt3K648FscG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EVdtEfwXpu-"},"outputs":[],"source":["# Creating Model and fitting it\n","%%time\n","model = KMeans(n_clusters = 3, random_state = 10).fit(X)\n","pred = model.fit_predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkA6gKNPYSnW"},"outputs":[],"source":["CS_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeXTYwxOXv6_"},"outputs":[],"source":["color1 = [ \"red\", \"blue\", \"Yellow\"]\n","l = [\"Customer segmentation\", 'Good Customers', 'Unsatisfied customers', 'Loyal customers']\n","plt.figure(figsize = (30, 8))\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(x = CS_df['Recency'], y = CS_df['price'], s = 70, hue =pred, palette = color1)\n","plt.legend(labels = l)\n","plt.title('recency v/s price', fontsize = 15)\n","\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(x = CS_df['Recency'], y = CS_df['Frequency'], s = 70, hue =pred, palette = color1)\n","plt.legend(labels = l)\n","plt.title('recency v/s frequency', fontsize = 15)\n","plt.show()"]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From the above clustering result we can see cleary 3 types of clusters are there**\n","- 1. Good customers: They are visiting the sites more frequently and revenue is good\n","- 2. Unsatisfied customers: They are stop visiting the site for a long time so we can assume that they are not satisfied with the service\n","- 3. Loyal customers: they are frequently visiting customers aswell as they are generating high revenue than that good customers </div>"],"metadata":{"id":"TR--UkauF6lM"}},{"cell_type":"markdown","metadata":{"id":"LB5f-zKgASii"},"source":["## &#10148; Amazon recommendation system</br>\n","### What Recommendation Systems Can Solve?\n","- It helps the consumer to find the best product.\n","- It helps websites to increase user engagement.\n","- It makes the contents more personalized.\n","- It helps websites to find the most relevant product for the consumer.\n","- Help item providers in delivering their items to the right user."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fFVZt2GAunG"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLAfSepB1i03"},"outputs":[],"source":["df3 = df[['userID', 'asin', 'overall']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qiqBenMJe0n"},"outputs":[],"source":["df3.rename(columns = {'asin':'productId', 'overall': 'ratings'}, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZNvqMs5J-sX"},"outputs":[],"source":["df3.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4aux_Wg1i5o"},"outputs":[],"source":["df3.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fbMf4le1i8U"},"outputs":[],"source":["df4=df3.iloc[:1000005,0:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnbfShqq1i-Y"},"outputs":[],"source":["df4.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uh4FwQNq1jAb"},"outputs":[],"source":["plt.figure(figsize = (15, 8))\n","sns.countplot(df4['ratings'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7I5wpUYFXhj4"},"outputs":[],"source":["print(\"\\nTotal no of ratings :\",df4.shape[0])\n","print(\"Total No of Users   :\", len(np.unique(df4.userID)))\n","print(\"Total No of products  :\", len(np.unique(df4.productId)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-656m_WLXhmk"},"outputs":[],"source":["top_rating = df4.groupby(by='userID')['ratings'].count().sort_values(ascending=False)[:10]\n","print('Top 10 users based on ratings: \\n',top_rating)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4GYK9heXhpF"},"outputs":[],"source":["new_df=df4.groupby(\"productId\").filter(lambda x:x['ratings'].count() >=50)\n","new_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6aDkm75Yd-t"},"outputs":[],"source":["new_df1=new_df.head(10000)\n","\n","ratings_matrix = new_df1.pivot_table(values='ratings', index='productId', columns='userID', fill_value=0)\n","ratings_matrix.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZF0oPHhSXhrl"},"outputs":[],"source":["print('Shape of the pivot table: ', ratings_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STlVB6xuXhuC"},"outputs":[],"source":["X = ratings_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrBSp3ewmIhA"},"outputs":[],"source":["new_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CE1rNs7H7w8o"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMlvGPNNXh0j"},"outputs":[],"source":["%%time\n","from sklearn.decomposition import TruncatedSVD       # used for dimensionality reduction\n","SVD = TruncatedSVD(n_components=5)\n","decomposed_matrix = SVD.fit_transform(X)\n","decomposed_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIAJ3TTCXh2t"},"outputs":[],"source":["%%time\n","correlation_matrix = np.corrcoef(decomposed_matrix)        # Return Pearson product-moment correlation coefficients.\n","correlation_matrix.shape"]},{"cell_type":"markdown","metadata":{"id":"rl5w9gQ45RQl"},"source":["The Pearson product-moment correlation coefficient (or Pearson correlation coefficient) is a measure of the strength of a linear association between two variables"]},{"cell_type":"code","source":["def recommend(s):\n","  l = list(X.index)\n","  h = l.index(s)\n","  i=X.index[h]\n","  correlation_product_ID = correlation_matrix[h]\n","  Recommend = list(X.index[correlation_product_ID > 0.05])\n","  Recommend.remove(i)\n","  print(Recommend[0:5])"],"metadata":{"id":"jP0dO3F61ksY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = 'B0000002ME'"],"metadata":{"id":"KE4Ne6Ej1wga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend(s)"],"metadata":{"id":"4Tb0C-V92_qH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <div class=\"alert alert-info\">Interpretation\n","**- From the above recomendation system we can see by using the correlation matrix the products are recomended based on the related product so this will help the customers to find the related products and it will generate the good revenue for the company aswell**"],"metadata":{"id":"v94x6V-zG5O0"}},{"cell_type":"markdown","metadata":{"id":"hv58W3fwE-7V"},"source":["## Conclusion\n","**- EDA**\n","- Count of reviews increasing over the period of time\n","- Revenue is increasing over the period of time\n","\n","**- SENTIMENTAL ANALYSIS**\n","- Model is able to detect and translate all the languages to English\n","- Model is able to Automate Sentiment Predictions\n","\n","**- CLUSTERING**\n","- Model is able to segregate top and bottom products\n","- Model is able to create segments based on customer perceptions\n","\n","**- PRODUCT RECOMMENDATION**\n","- Model is able to recommend related products based on customer purchase\n","- Model is able to forecast future trend of the sentiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aINGO4FOFCb5"},"outputs":[],"source":[]}]}